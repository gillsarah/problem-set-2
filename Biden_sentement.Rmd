---
title: "Gill_Sarah_ML_PS2"
author: "Sarah Gill"
date: "1/27/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/problem-set-2")

mse <- function(data, ...) {
  UseMethod("mse")
}

#' @export
#' @rdname mse
mse.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {

  metric_summarizer(
    metric_nm = "mse",
    metric_fn = mse_vec,
    data = data,
    truth = !! rlang::enquo(truth),
    estimate = !! rlang::enquo(estimate),
    na_rm = na_rm,
    ...
  )

}


#' @export
#' @rdname mse
mse_vec <- function(truth, estimate, na_rm = TRUE, ...) {

  mse_impl <- function(truth, estimate) {
    mean((truth - estimate) ^ 2)
  }

  metric_vec_template(
    metric_impl = mse_impl,
    truth = truth,
    estimate = estimate,
    na_rm = na_rm,
    cls = "numeric",
    ...
  )

}
```

```{r}
library(readr)
#library(rcfss) #breaks tidy
library(boot)
library(tidyverse)
library(broom)
library(dplyr)
library(rsample)
library(yardstick)
set.seed(1234)
```


1. Estimate the MSE of the model using the traditional approach. That is, fit the linear
regression model using the entire dataset and calculate the mean squared error for the entire dataset.
```{r}

nes2008_df <- read_csv("nes2008.csv")

regn_model <- glm(biden ~ female + age + educ + dem + rep, data = nes2008_df)

summary(regn_model)

(mse <- augment(regn_model, newdata = nes2008_df) %>%
  mse(truth = biden, estimate = .fitted))

```
Present and discuss your results at a simple, high level.

mse = 395.27

This seems large, especially given the values that we are estimating (mean 62, variance 505). The mse, the average of the squared difference between estimates and the data points.
```{r}
max(nes2008_df$biden)
min(nes2008_df$biden)
var(nes2008_df$biden)
mean(nes2008_df$biden)
```



2. Calculate the test MSE of the model using the simple holdout validation approach.
```{r simple_holdout}

#Split the sample set into a training set (50%) and a holdout set (50%). Be sure to set your seed prior to this part of your code to guarantee reproducibility of results.
nes_split <- initial_split(data = nes2008_df, 
                            prop = 0.5) #split the data in half
nes_train <- training(nes_split) #random subsample to train
nes_test <- testing(nes_split)

#Fit the linear regression model using only the training observations.
nes_lm <- glm(biden~female+age+educ+dem+rep, data = nes_train) #fit model on training data

#(train_mse <- augment(nes_lm, newdata = nes_train) %>%
#  mse(truth = biden, estimate = .fitted))


#Calculate the MSE using only the test set observations.
(test_mse <- augment(nes_lm, newdata = nes_test) %>%
  mse(truth = biden, estimate = .fitted))

```

mse = 389.16
(recall mse from the simple general linear model was 395.27)

This is a slight improvement over the mse for the standard glm, however this is not expected and is likely because of the particular draw (see 3). Since we are modeling using only half of the data, then testing it on the other half we may expect a higher mse given that this estimate is generated from a smaller dataset, and unlike in 1 it is compared to different data than it was produced from.



3. Repeat the simple validation set approach from the previous question 1000 times, using 1000 different splits of the observations into a training set and a test/validation set. 
```{r mse_list}

x <- 1
mse_list <- c()

repeat{
  nes_split <- initial_split(data = nes2008_df, 
                            prop = 0.5) #split the data in half
  nes_train <- training(nes_split) #random subsample to train
  nes_test <- testing(nes_split)

  #Fit the linear regression model using only the training observations.
  nes_lm <- glm(biden~female+age+educ+dem+rep, data = nes_train) #fit model on training   data
  mse <- augment(nes_lm, newdata = nes_test) %>%
    mse(truth = biden, estimate = .fitted)%>% 
    select(.estimate)%>%
    as.numeric()
  mse_list <- append(mse_list,mse)
  x= x+1
  
  if (x == 1000){
    break
  }
}
 #source https://www.datamentor.io/r-programming/repeat-loop/
mean(mse_list)

```
Visualize your results as a sampling distribution ( hint: think histogram or density plots). Comment on the results obtained.

```{r dependson=mse_list}
data <- data.frame(mse = mse_list)

ggplot(data, aes(x = mse))+
  geom_histogram(binwidth = 1, alpha = 0.75) 
  

```

When we iteratively split the data, run a regression on each split and extract the mse we can see that the mse estimates fall in a roughly normal distribution, centered on the mean 398.37. 

Note that this mean is similar to the mse from the simple linear regression: 395.27



4. Compare the estimated parameters and standard errors from the original model in question 1 (the model estimated using all of the available data) to parameters and standard errors estimated using the bootstrap (B = 1000). Comparison should include, at a minimum, both numeric output as well as discussion on differences, similarities, etc. Talk also about the conceptual use and impact of bootstrapping.

```{r bootstrap}
#broom::tidy()

regn_model <- lm(biden ~ female + age + educ + dem + rep, data = nes2008_df)
tidy(regn_model)


# bootstrapped estimates of the parameter estimates and standard errors
lm_coefs <- function(splits, ...) {
  ## use `analysis` or `as.data.frame` to get the analysis data
  mod <- lm(..., data = analysis(splits))
  tidy(mod)
}

biden_boot <- nes2008_df %>%
  bootstraps(1000) %>%
  mutate(coef = map(splits, lm_coefs, as.formula(biden ~ female + age + educ + dem + rep)))


biden_boot %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(.estimate = mean(estimate),
            .se = sd(estimate, na.rm = TRUE))


```
Bootstrap:                      Linear Regression:
Estimates:                      Estimates:

(Intercept)	58.7844692		      (Intercept)	58.81125899	
age	        0.0484685			      age	        0.04825892	
dem	        15.4786044	        dem	        15.42425563		
educ	      -0.3448924		      educ	      -0.34533479
female	    4.0856644			      female	    4.10323009
rep	        -15.8100846	        rep	        -15.84950614


The bootstrap and linear regression estimates are very similar, often only different at the tens or hundredths place.

Bootstrap standard errors are larger though (e.g. the SE on the coefficient estimate for age is 0.0282474  for the liner mode and 0.02897384 for the bootstrap). However, some SEs are actually larger in the linear model than the bootstrap (e.g. females is 0.9482286 in the linear model and 0.94451110 in the bootstrap). The larger standard errors in bootstrap makes sense because we are not assuming a distribution. This is useful if we are not prepared to make an assumption about the distribution of our population. 

